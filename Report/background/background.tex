
\chapter{Background Theory}

\label{ch:background}

\section{Introduction}
\label{sec:background_introduction}
% In this chapter, we explain some concepts and take a look at some background work related to this
% study. Firstly, we review some research in the use of the naive Bayes Classifier for text
% classification. Then we take a look at some background in topic modelling, specifically the Latent
% Dirichlet Allocation

Automatic Text Classification or Text Categorization is a rapidly growing field in Machine Learning
and Natural Language Processing. This mainly due to the amount of electronic data we currently
generate. The main task is to assign one or more classes to a given text document. Applications of
text classification include \textit{Email Spam Detection} and \textit{Language
Detection}. The former involves trying to distinguish spam emails from legitimate ones while the
latter involves the identification of the language a document was written in.

However, this study makes use of classification techniques for data filtration (removing irrelevant
documents from a list of documents, similar to spam filtering), topic modelling (extracting topics
from a list of documents) and sentiment analysis (predicting the sentiment of the author of a
document). This chapter explains a few background concepts and reviews some relevant research
previously done in this area.

\section{Naive Bayes Classifier}
\label{sec:bg_text_classification}
The naive Bayes classifier is the simplest classifier that can be used and this is due to the fact
that is is based on simple Bayes Theorem. It is a probabilistic classifier which
assumes that all features of the documents are independent of each other.

Bayes theorem states that the probability of $A$ given $B$ is the probability of $B$ given $A$ times
the probability of $A$ divided by the probability of $B$. Mathematically, this is written as:
\begin{equation}
  p(A|B) = \frac{p(B|A)p(A)}{p(B)}
\end{equation}

Applying this logic to text classification, the probability that a document $d_i \in D$ belongs to a
class $c$ is denoted as:
\begin{equation}
  p(c|d_i) = \frac{p(d_i|c)p(c)}{p(d_i)}
\end{equation}

Although other techniques like Maximum Entropy, Random Forests or Support Vector Machines tend to
perform better, a naive Bayes classifier will require less memory and CPU cycles. It is also
computationally less complex and simpler to implement. With regard to
performance,~\cite{huangLuLing2003} showed using multiple datasets from~\cite{blakeMerz1998} that
the naive Bayes classifier in many cases performs as good as other complex classifiers
and~\cite{zhang2004} goes further to explain why it performs well. Other studies have also found
Bayesian classifiers to be effective without being affected by its simple independence
assumption~\cite{langley1992analysis,manning2008}.

\section{Topic Modelling}
\label{sec:bg_topic_modelling}
Topic Modelling is a process by which abstract topics/themes are extracted from a collection of
documents. This process is usually carried out with the aid of topic models, a suite of algorithms
used for topic modelling. It has been applied in a variety of fields like Software Analysis
where~\cite{linstead2009software} used topic modelling to find topics embedded in code
and~\cite{gethers2010using} used topic modelling to capture coupling among
classes.~\cite{kireyev2009applications} applied topic models on disaster related data from Twitter
in an effort to determine what topics were discussed within the time span of a natural
disaster.~\cite{hospedales2009markov} introduced a new topic model that can be used to analyze
videos with complex and crowded scenes in other to discover regularities in the videos. A system
built on such model will be able to answer a question like ``What interesting events happened in the
last 5 hours''. Other fields include Audio Analysis~\cite{smaragdis2009topic}, Influence
modelling~\cite{gerrish2009modeling}, Finance~\cite{doyle2009financial}, Writer
Identification~\cite{bhardwaj2009writer} and many more.

There are a number of topic models but the two main ones are \textbf{\textit{Latent Semantic
Indexing}} (LSI) and \textbf{\textit{Latent Dirichlet Allocation}} (LDA) and we discuss them further
in the following sections.


\subsection{Latent Semantic Indexing}
\label{sub:bg_lsa}
Latent Semantic Indexing, sometimes referred to as \textit{Latent Semantic Analysis}, is an indexing
technique leverages matrix-algebra computations\footnote{Specifically, it uses Singular Value
Decomposition which is a factorization of a complex matrix. See
\url{http://en.wikipedia.org/wiki/Singular_value_decomposition}} to identify any patterns in
relationships between a collection of text documents. It works based on the assumption that words
used in the same context tend to have homogeneous
meanings~\cite{deerwester1990indexing,dumais2004latent,landauer2006latent}.

\subsection{Latent Dirichlet allocation}
\label{sub:bg_lda}
lda is blah blh blah

% \section{Sentiment Analysis}
% \label{sec:bg_sentiment_analysis}

\section{Model Evaluation}
\label{sec:bg_model_evaluation}

